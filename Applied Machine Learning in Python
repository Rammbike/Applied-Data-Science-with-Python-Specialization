Assignment 1 - Introduction to Machine Learning
For this assignment, you will be using the Breast Cancer Wisconsin (Diagnostic) Database to create a classifier that can help diagnose patients. First, read through the description of the dataset (below).

import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer
​
cancer = load_breast_cancer()
​
#print(cancer.DESCR) # Print the data set description
The object returned by load_breast_cancer() is a scikit-learn Bunch object, which is similar to a dictionary.

cancer.keys()
dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names'])
Question 0 (Example)
How many features does the breast cancer dataset have?

This function should return an integer.

# You should write your whole answer within the function provided. The autograder will call
# this function and compare the return value against the correct solution value
def answer_zero():
    # This function returns the number of features of the breast cancer dataset, which is an integer. 
    # The assignment question description will tell you the general format the autograder is expecting
    return len(cancer['feature_names'])
​
# You can examine what your function returns by calling it in the cell. If you have questions
# about the assignment formats, check out the discussion forums for any FAQs
answer_zero() 
30
Question 1
Scikit-learn works with lists, numpy arrays, scipy-sparse matrices, and pandas DataFrames, so converting the dataset to a DataFrame is not necessary for training this model. Using a DataFrame does however help make many things easier such as munging data, so let's practice creating a classifier with a pandas DataFrame.

Convert the sklearn.dataset cancer to a DataFrame.

This function should return a (569, 31) DataFrame with

columns =

['mean radius', 'mean texture', 'mean perimeter', 'mean area',
'mean smoothness', 'mean compactness', 'mean concavity',
'mean concave points', 'mean symmetry', 'mean fractal dimension',
'radius error', 'texture error', 'perimeter error', 'area error',
'smoothness error', 'compactness error', 'concavity error',
'concave points error', 'symmetry error', 'fractal dimension error',
'worst radius', 'worst texture', 'worst perimeter', 'worst area',
'worst smoothness', 'worst compactness', 'worst concavity',
'worst concave points', 'worst symmetry', 'worst fractal dimension',
'target']
and index =

RangeIndex(start=0, stop=569, step=1)
def answer_one():
    df = pd.DataFrame(cancer.data, columns=cancer.feature_names)
    df['target'] = pd.Series(cancer.target)
    return df
Question 2
What is the class distribution? (i.e. how many instances of malignant (encoded 0) and how many benign (encoded 1)?)

This function should return a Series named target of length 2 with integer values and index = ['malignant', 'benign']

def answer_two():
    cancerdf = answer_one()
    label = {0: 'malignant', 1: 'benign'}
    cancerdf['target'] = cancerdf['target'].map(label)
    return cancerdf['target'].value_counts()
benign       357
malignant    212
Name: target, dtype: int64
Question 3
Split the DataFrame into X (the data) and y (the labels).

This function should return a tuple of length 2: (X, y), where

X, a pandas DataFrame, has shape (569, 30)
y, a pandas Series, has shape (569,).
def answer_three():
    cancerdf = answer_one()
    X = cancerdf.iloc[:, :30]
    y = cancerdf.iloc[:, -1]
    
    return X, y
​
Question 4
Using train_test_split, split X and y into training and test sets (X_train, X_test, y_train, and y_test).

Set the random number generator state to 0 using random_state=0 to make sure your results match the autograder!

This function should return a tuple of length 4: (X_train, X_test, y_train, y_test), where

X_train has shape (426, 30)
X_test has shape (143, 30)
y_train has shape (426,)
y_test has shape (143,)
from sklearn.model_selection import train_test_split
​
def answer_four():
    X, y = answer_three()
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
    
    return X_train, X_test, y_train, y_test
Question 5
Using KNeighborsClassifier, fit a k-nearest neighbors (knn) classifier with X_train, y_train and using one nearest neighbor (n_neighbors = 1).

This function should return a sklearn.neighbors.classification.KNeighborsClassifier.

from sklearn.neighbors import KNeighborsClassifier
​
def answer_five():
    X_train, X_test, y_train, y_test = answer_four()
    return KNeighborsClassifier(n_neighbors = 1).fit(X_train, y_train)
Question 6
Using your knn classifier, predict the class label using the mean value for each feature.

Hint: You can use cancerdf.mean()[:-1].values.reshape(1, -1) which gets the mean value for each feature, ignores the target column, and reshapes the data from 1 dimension to 2 (necessary for the precict method of KNeighborsClassifier).

This function should return a numpy array either array([ 0.]) or array([ 1.])

def answer_six():
    cancerdf = answer_one()
    knn = answer_five()
    means = cancerdf.mean()[:-1].values.reshape(1, -1)
    return [knn.predict(mean) for mean in means]
/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.
  DeprecationWarning)
[array([1])]
Question 7
Using your knn classifier, predict the class labels for the test set X_test.

This function should return a numpy array with shape (143,) and values either 0.0 or 1.0.

def answer_seven():
    X_train, X_test, y_train, y_test = answer_four()
    knn = answer_five()
    knn.predict(X_test)
    return knn.predict(X_test)
Question 8
Find the score (mean accuracy) of your knn classifier using X_test and y_test.

This function should return a float between 0 and 1

def answer_eight():
    X_train, X_test, y_train, y_test = answer_four()
    knn = answer_five()
    return knn.score(X_test, y_test)
Optional plot
Try using the plotting function below to visualize the differet predicition scores between training and test sets, as well as malignant and benign cells.

def accuracy_plot():
    import matplotlib.pyplot as plt
​
    %matplotlib notebook
​
    X_train, X_test, y_train, y_test = answer_four()
​
    # Find the training and testing accuracies by target value (i.e. malignant, benign)
    mal_train_X = X_train[y_train==0]
    mal_train_y = y_train[y_train==0]
    ben_train_X = X_train[y_train==1]
    ben_train_y = y_train[y_train==1]
​
    mal_test_X = X_test[y_test==0]
    mal_test_y = y_test[y_test==0]
    ben_test_X = X_test[y_test==1]
    ben_test_y = y_test[y_test==1]
​
    knn = answer_five()
​
    scores = [knn.score(mal_train_X, mal_train_y), knn.score(ben_train_X, ben_train_y), 
              knn.score(mal_test_X, mal_test_y), knn.score(ben_test_X, ben_test_y)]
​
​
    plt.figure()
​
    # Plot the scores as a bar chart
    bars = plt.bar(np.arange(4), scores, color=['#4c72b0','#4c72b0','#55a868','#55a868'])
​
    # directly label the score onto the bars
    for bar in bars:
        height = bar.get_height()
        plt.gca().text(bar.get_x() + bar.get_width()/2, height*.90, '{0:.{1}f}'.format(height, 2), 
                     ha='center', color='w', fontsize=11)
​
    # remove all the ticks (both axes), and tick labels on the Y axis
    plt.tick_params(top='off', bottom='off', left='off', right='off', labelleft='off', labelbottom='on')
​
    # remove the frame of the chart
    for spine in plt.gca().spines.values():
        spine.set_visible(False)
​
    plt.xticks([0,1,2,3], ['Malignant\nTraining', 'Benign\nTraining', 'Malignant\nTest', 'Benign\nTest'], alpha=0.8);
    plt.title('Training and Test Accuracies for Malignant and Benign Cells', alpha=0.8)
Uncomment the plotting function to see the visualization.

Comment out the plotting function when submitting your notebook for grading.

#accuracy_plot() 













































Assignment 2
In this assignment you'll explore the relationship between model complexity and generalization performance, by adjusting key parameters of various supervised learning models. Part 1 of this assignment will look at regression and Part 2 will look at classification.

Part 1 - Regression
First, run the following block to set up the variables needed for later sections.

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
​
​
np.random.seed(0)
n = 15
x = np.linspace(0,10,n) + np.random.randn(n)/5
y = np.sin(x)+x/6 + np.random.randn(n)/10
​
​
X_train, X_test, y_train, y_test = train_test_split(x, y, random_state=0)
​
# You can use this function to help you visualize the dataset by
# plotting a scatterplot of the data points
# in the training and test sets.
def part1_scatter():
    import matplotlib.pyplot as plt
    %matplotlib notebook
    plt.figure()
    plt.scatter(X_train, y_train, label='training data')
    plt.scatter(X_test, y_test, label='test data')
    plt.legend(loc=4);
    
    
# NOTE: Uncomment the function below to visualize the data, but be sure 
# to **re-comment it before submitting this assignment to the autograder**.   
#part1_scatter()
X_train
array([ 10.08877265,   3.23065446,   1.62431903,   9.31004929,
         7.17166586,   4.96972856,   8.14799756,   2.59103578,
         0.35281047,   3.375973  ,   8.72363612])
Question 1
Write a function that fits a polynomial LinearRegression model on the training data X_train for degrees 1, 3, 6, and 9. (Use PolynomialFeatures in sklearn.preprocessing to create the polynomial features and then fit a linear regression model) For each model, find 100 predicted values over the interval x = 0 to 10 (e.g. np.linspace(0,10,100)) and store this in a numpy array. The first row of this array should correspond to the output from the model trained on degree 1, the second row degree 3, the third row degree 6, and the fourth row degree 9.



The figure above shows the fitted models plotted on top of the original data (using plot_one()).


This function should return a numpy array with shape (4, 100)

def answer_one():
    from sklearn.linear_model import LinearRegression
    from sklearn.preprocessing import PolynomialFeatures
    final = np.zeros((4, 100))
    for i, degree in enumerate([1, 3, 6, 9]):
        poly = PolynomialFeatures(degree=degree)
        X_train_poly = poly.fit_transform(X_train.reshape(-1, 1))
        linreg = LinearRegression().fit(X_train_poly, y_train)
        results = linreg.predict(poly.fit_transform(np.linspace(0, 10, 100).reshape(100, 1)))
        final[i, :] = results
    return final
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=3)
X_train_poly = poly.fit_transform(X_train.reshape(-1, 1))
X_train_poly
array([[  1.00000000e+00,   1.00887726e+01,   1.01783334e+02,
          1.02686891e+03],
       [  1.00000000e+00,   3.23065446e+00,   1.04371282e+01,
          3.37187547e+01],
       [  1.00000000e+00,   1.62431903e+00,   2.63841230e+00,
          4.28562329e+00],
       [  1.00000000e+00,   9.31004929e+00,   8.66770178e+01,
          8.06967308e+02],
       [  1.00000000e+00,   7.17166586e+00,   5.14327912e+01,
          3.68858792e+02],
       [  1.00000000e+00,   4.96972856e+00,   2.46982019e+01,
          1.22743360e+02],
       [  1.00000000e+00,   8.14799756e+00,   6.63898642e+01,
          5.40944452e+02],
       [  1.00000000e+00,   2.59103578e+00,   6.71346643e+00,
          1.73948317e+01],
       [  1.00000000e+00,   3.52810469e-01,   1.24475227e-01,
          4.39161633e-02],
       [  1.00000000e+00,   3.37597300e+00,   1.13971937e+01,
          3.84766180e+01],
       [  1.00000000e+00,   8.72363612e+00,   7.61018271e+01,
          6.63884647e+02]])
​
​
# feel free to use the function plot_one() to replicate the figure 
# from the prompt once you have completed question one
def plot_one(degree_predictions):
    import matplotlib.pyplot as plt
    %matplotlib notebook
    plt.figure(figsize=(10,5))
    plt.plot(X_train, y_train, 'o', label='training data', markersize=10)
    plt.plot(X_test, y_test, 'o', label='test data', markersize=10)
    for i,degree in enumerate([1,3,6,9]):
        plt.plot(np.linspace(0,10,100), degree_predictions[i], alpha=0.8, lw=2, label='degree={}'.format(degree))
    plt.ylim(-1,2.5)
    plt.legend(loc=4)
​
#plot_one(answer_one())
Question 2
Write a function that fits a polynomial LinearRegression model on the training data X_train for degrees 0 through 9. For each model compute the R2R2 (coefficient of determination) regression score on the training data as well as the the test data, and return both of these arrays in a tuple.

This function should return one tuple of numpy arrays (r2_train, r2_test). Both arrays should have shape (10,)

def answer_two():
    from sklearn.linear_model import LinearRegression
    from sklearn.preprocessing import PolynomialFeatures
    from sklearn.metrics.regression import r2_score
​
    r2_train = np.zeros(10)
    r2_test = np.zeros(10)
​
    for i, degree in enumerate(range(10)):
        poly = PolynomialFeatures(degree = degree)
        
        
        X_train_poly = poly.fit_transform(X_train.reshape(-1, 1))
        linreg = LinearRegression().fit(X_train_poly, y_train)
        r2_train[i] = linreg.score(X_train_poly, y_train)
​
        
        X_test_poly = poly.fit_transform(X_test.reshape(-1, 1))
        r2_test[i] = linreg.score(X_test_poly, y_test)
​
    return (r2_train, r2_test)
​
    
Question 3
Based on the R2R2 scores from question 2 (degree levels 0 through 9), what degree level corresponds to a model that is underfitting? What degree level corresponds to a model that is overfitting? What choice of degree level would provide a model with good generalization performance on this dataset?

Hint: Try plotting the R2R2 scores from question 2 to visualize the relationship between degree level and R2R2. Remember to comment out the import matplotlib line before submission.

This function should return one tuple with the degree values in this order: (Underfitting, Overfitting, Good_Generalization). There might be multiple correct solutions, however, you only need to return one possible solution, for example, (1,2,3).

r2_scores = answer_two()
    df = pd.DataFrame({'training_score':r2_scores[0], 'test_score':r2_scores[1]})
    df['diff'] = df['training_score'] - df['test_score']

    Good_Generalization  = df['diff'].idxmin()
    Overfitting = df['training_score'].idxmax()
    Underfitting = df['training_score'].idxmin()
def answer_three():
    
    r2_scores = answer_two()
    df = pd.DataFrame({'training_score':r2_scores[0], 'test_score':r2_scores[1]})
    df['diff'] = df['training_score'] - df['test_score']
​
    Good_Generalization  = df['diff'].idxmin()
    Overfitting = df['training_score'].idxmax()
    Underfitting = df['training_score'].idxmin()
    
    return (Underfitting, Overfitting, Good_Generalization)
df
r2_scores = answer_two()
df = pd.DataFrame({'training_score':r2_scores[0], 'test_score':r2_scores[1]})
df['diff'] = df['training_score'] - df['test_score']
​
Good_Generalization  = df['diff'].idxmin()
Overfitting = df['training_score'].idxmax()
Underfitting = df['training_score'].idxmin()
df
test_score	training_score	diff
0	-0.478086	0.000000	0.478086
1	-0.452371	0.429246	0.881617
2	-0.068570	0.451100	0.519670
3	0.005331	0.587200	0.581868
4	0.730049	0.919419	0.189370
5	0.877083	0.975786	0.098703
6	0.921409	0.990182	0.068773
7	0.920215	0.993525	0.073310
8	0.632480	0.996375	0.363896
9	-0.645254	0.998037	1.643291
Question 4
Training models on high degree polynomial features can result in overly complex models that overfit, so we often use regularized versions of the model to constrain model complexity, as we saw with Ridge and Lasso linear regression.

For this question, train two models: a non-regularized LinearRegression model (default parameters) and a regularized Lasso Regression model (with parameters alpha=0.01, max_iter=10000) both on polynomial features of degree 12. Return the R2R2 score for both the LinearRegression and Lasso model's test sets.

This function should return one tuple (LinearRegression_R2_test_score, Lasso_R2_test_score)

def answer_four():
    from sklearn.preprocessing import PolynomialFeatures
    from sklearn.linear_model import Lasso, LinearRegression
    from sklearn.metrics.regression import r2_score
​
    poly = PolynomialFeatures(degree=12)
    X_train_poly = poly.fit_transform(X_train.reshape(-1, 1))
    X_test_poly = poly.fit_transform(X_test.reshape(-1, 1))
​
    linreg = LinearRegression().fit(X_train_poly, y_train)
    LinearRegression_R2_test_score = linreg.score(X_test_poly, y_test)
​
    lassoreg = Lasso(alpha=0.01, max_iter=10000).fit(X_train_poly, y_train)
    Lasso_R2_test_score = lassoreg.score(X_test_poly, y_test)
​
    return (LinearRegression_R2_test_score, Lasso_R2_test_score)
​
Part 2 - Classification
Here's an application of machine learning that could save your life! For this section of the assignment we will be working with the UCI Mushroom Data Set stored in readonly/mushrooms.csv. The data will be used to train a model to predict whether or not a mushroom is poisonous. The following attributes are provided:

Attribute Information:

cap-shape: bell=b, conical=c, convex=x, flat=f, knobbed=k, sunken=s
cap-surface: fibrous=f, grooves=g, scaly=y, smooth=s
cap-color: brown=n, buff=b, cinnamon=c, gray=g, green=r, pink=p, purple=u, red=e, white=w, yellow=y
bruises?: bruises=t, no=f
odor: almond=a, anise=l, creosote=c, fishy=y, foul=f, musty=m, none=n, pungent=p, spicy=s
gill-attachment: attached=a, descending=d, free=f, notched=n
gill-spacing: close=c, crowded=w, distant=d
gill-size: broad=b, narrow=n
gill-color: black=k, brown=n, buff=b, chocolate=h, gray=g, green=r, orange=o, pink=p, purple=u, red=e, white=w, yellow=y
stalk-shape: enlarging=e, tapering=t
stalk-root: bulbous=b, club=c, cup=u, equal=e, rhizomorphs=z, rooted=r, missing=?
stalk-surface-above-ring: fibrous=f, scaly=y, silky=k, smooth=s
stalk-surface-below-ring: fibrous=f, scaly=y, silky=k, smooth=s
stalk-color-above-ring: brown=n, buff=b, cinnamon=c, gray=g, orange=o, pink=p, red=e, white=w, yellow=y
stalk-color-below-ring: brown=n, buff=b, cinnamon=c, gray=g, orange=o, pink=p, red=e, white=w, yellow=y
veil-type: partial=p, universal=u
veil-color: brown=n, orange=o, white=w, yellow=y
ring-number: none=n, one=o, two=t
ring-type: cobwebby=c, evanescent=e, flaring=f, large=l, none=n, pendant=p, sheathing=s, zone=z
spore-print-color: black=k, brown=n, buff=b, chocolate=h, green=r, orange=o, purple=u, white=w, yellow=y
population: abundant=a, clustered=c, numerous=n, scattered=s, several=v, solitary=y
habitat: grasses=g, leaves=l, meadows=m, paths=p, urban=u, waste=w, woods=d


The data in the mushrooms dataset is currently encoded with strings. These values will need to be encoded to numeric to work with sklearn. We'll use pd.get_dummies to convert the categorical variables into indicator variables.

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
​
​
mush_df = pd.read_csv('mushrooms.csv')
mush_df2 = pd.get_dummies(mush_df)
​
X_mush = mush_df2.iloc[:,2:]
y_mush = mush_df2.iloc[:,1]
​
# use the variables X_train2, y_train2 for Question 5
X_train2, X_test2, y_train2, y_test2 = train_test_split(X_mush, y_mush, random_state=0)
​
# For performance reasons in Questions 6 and 7, we will create a smaller version of the
# entire mushroom dataset for use in those questions.  For simplicity we'll just re-use
# the 25% test split created above as the representative subset.
#
# Use the variables X_subset, y_subset for Questions 6 and 7.
X_subset = X_test2
y_subset = y_test2
Question 5
Using X_train2 and y_train2 from the preceeding cell, train a DecisionTreeClassifier with default parameters and random_state=0. What are the 5 most important features found by the decision tree?

As a reminder, the feature names are available in the X_train2.columns property, and the order of the features in X_train2.columns matches the order of the feature importance values in the classifier's feature_importances_ property.

This function should return a list of length 5 containing the feature names in descending order of importance.

Note: remember that you also need to set random_state in the DecisionTreeClassifier.

def answer_five():
    from sklearn.tree import DecisionTreeClassifier
​
    importance = []
​
    clf = DecisionTreeClassifier(random_state = 0)
    clf = clf.fit(X_train2, y_train2)
    result = dict(zip( X_train2.columns, clf.feature_importances_))
    for v, k in sorted(result.items(), key = lambda x: x[1], reverse=True):
        importance.append(v)
    
​
    return importance[:5]
​
Question 6
For this question, we're going to use the validation_curve function in sklearn.model_selection to determine training and test scores for a Support Vector Classifier (SVC) with varying parameter values. Recall that the validation_curve function, in addition to taking an initialized unfitted classifier object, takes a dataset as input and does its own internal train-test splits to compute results.

Because creating a validation curve requires fitting multiple models, for performance reasons this question will use just a subset of the original mushroom dataset: please use the variables X_subset and y_subset as input to the validation curve function (instead of X_mush and y_mush) to reduce computation time.

The initialized unfitted classifier object we'll be using is a Support Vector Classifier with radial basis kernel. So your first step is to create an SVC object with default parameters (i.e. kernel='rbf', C=1) and random_state=0. Recall that the kernel width of the RBF kernel is controlled using the gamma parameter.

With this classifier, and the dataset in X_subset, y_subset, explore the effect of gamma on classifier accuracy by using the validation_curve function to find the training and test scores for 6 values of gamma from 0.0001 to 10 (i.e. np.logspace(-4,1,6)). Recall that you can specify what scoring metric you want validation_curve to use by setting the "scoring" parameter. In this case, we want to use "accuracy" as the scoring metric.

For each level of gamma, validation_curve will fit 3 models on different subsets of the data, returning two 6x3 (6 levels of gamma x 3 fits per level) arrays of the scores for the training and test sets.

Find the mean score across the three models for each level of gamma for both arrays, creating two arrays of length 6, and return a tuple with the two arrays.

e.g.

if one of your array of scores is

array([[ 0.5,  0.4,  0.6],
       [ 0.7,  0.8,  0.7],
       [ 0.9,  0.8,  0.8],
       [ 0.8,  0.7,  0.8],
       [ 0.7,  0.6,  0.6],
       [ 0.4,  0.6,  0.5]])
it should then become

array([ 0.5,  0.73333333,  0.83333333,  0.76666667,  0.63333333, 0.5])
This function should return one tuple of numpy arrays (training_scores, test_scores) where each array in the tuple has shape (6,).

train_scores, test_scores
def answer_six():
    from sklearn.svm import SVC
    from sklearn.model_selection import validation_curve
​
    clf = SVC(kernel='rbf', C=1, random_state = 0).fit(X_subset, y_subset)
    gamma = np.logspace(-4,1,6)
    train_scores, test_scores = validation_curve(SVC(), X_subset, y_subset, param_name = 'gamma', param_range = gamma, cv = 3)
​
    return (train_scores.mean(axis=1), test_scores.mean(axis=1))
train_scores
from sklearn.svm import SVC
from sklearn.model_selection import validation_curve
clf = SVC(kernel='rbf', C=1, random_state = 0).fit(X_subset, y_subset)
gamma = np.logspace(-4,1,6)
train_scores, test_scores = validation_curve(SVC(), X_subset, y_subset, param_name = 'gamma', param_range = gamma, cv = 3)
a = train_scores.mean(axis=1)
b = test_scores.mean(axis=1)
​
Question 7
Based on the scores from question 6, what gamma value corresponds to a model that is underfitting (and has the worst test set accuracy)? What gamma value corresponds to a model that is overfitting (and has the worst test set accuracy)? What choice of gamma would be the best choice for a model with good generalization performance on this dataset (high accuracy on both training and test set)?

Hint: Try plotting the scores from question 6 to visualize the relationship between gamma and accuracy. Remember to comment out the import matplotlib line before submission.

This function should return one tuple with the degree values in this order: (Underfitting, Overfitting, Good_Generalization) Please note there is only one correct solution.

def answer_seven():
    train_scores, test_scores = answer_six()
    gamma = np.logspace(-4,1,6)
    df = pd.DataFrame(train_scores, test_scores).reset_index()
    df.columns = ['train', 'test']
    df.set_index(gamma, inplace=True)
    df['diff'] = abs(df['train'] - df['test'])
​
    under = df
    under = under[under['train'] < under['test']]
    underfitting = under['test'].argmin()
    overfitting = df['diff'].argmax()
    good = df['diff'].argmin()
    
    return (underfitting, overfitting, good)
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    Assignment 3 - Evaluation
In this assignment you will train several models and evaluate how effectively they predict instances of fraud using data based on this dataset from Kaggle.

Each row in fraud_data.csv corresponds to a credit card transaction. Features include confidential variables V1 through V28 as well as Amount which is the amount of the transaction.

The target is stored in the class column, where a value of 1 corresponds to an instance of fraud and 0 corresponds to an instance of not fraud.

import numpy as np
import pandas as pd
Question 1
Import the data from fraud_data.csv. What percentage of the observations in the dataset are instances of fraud?

This function should return a float between 0 and 1.

def answer_one():
    df = pd.read_csv('fraud_data.csv')
    notfraud, fraud = df['Class'].value_counts()
    total = fraud + notfraud  
    percentage = fraud / total   
    return percentage
# Use X_train, X_test, y_train, y_test for all of the following questions
from sklearn.model_selection import train_test_split
​
df = pd.read_csv('fraud_data.csv')
​
X = df.iloc[:,:-1]
y = df.iloc[:,-1]
​
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
Question 2
Using X_train, X_test, y_train, and y_test (as defined above), train a dummy classifier that classifies everything as the majority class of the training data. What is the accuracy of this classifier? What is the recall?

This function should a return a tuple with two floats, i.e. (accuracy score, recall score).

def answer_two():
    from sklearn.dummy import DummyClassifier
    from sklearn.metrics import recall_score
​
    clf = DummyClassifier().fit(X_train, y_train)
    prediction = clf.predict(X_test)
    accuracy = clf.score(X_test, y_test)
    recall = recall_score(prediction, y_test)
​
    return (accuracy, recall)
Question 3
Using X_train, X_test, y_train, y_test (as defined above), train a SVC classifer using the default parameters. What is the accuracy, recall, and precision of this classifier?

This function should a return a tuple with three floats, i.e. (accuracy score, recall score, precision score).

def answer_three():
    from sklearn.metrics import recall_score, precision_score
    from sklearn.svm import SVC
​
    clf = SVC().fit(X_train, y_train)
    prediction = clf.predict(X_test)
    accuracy = clf.score(X_test, y_test)
    recall_score = recall_score(prediction, y_test)
    precision = precision_score(prediction, y_test)
​
    return accuracy, precision, recall_score
​
​
Question 4
Using the SVC classifier with parameters {'C': 1e9, 'gamma': 1e-07}, what is the confusion matrix when using a threshold of -220 on the decision function. Use X_test and y_test.

This function should return a confusion matrix, a 2x2 numpy array with 4 integers.

def answer_four():
    from sklearn.metrics import confusion_matrix
    from sklearn.svm import SVC
​
    clf = SVC(C=1e9, gamma=1e-07).fit(X_train, y_train)
    y_score = clf.decision_function(X_test) > -220 
    confusion = confusion_matrix(y_test, y_score)
​
    return confusion
Question 5
Train a logisitic regression classifier with default parameters using X_train and y_train.

For the logisitic regression classifier, create a precision recall curve and a roc curve using y_test and the probability estimates for X_test (probability it is fraud).

Looking at the precision recall curve, what is the recall when the precision is 0.75?

Looking at the roc curve, what is the true positive rate when the false positive rate is 0.16?

This function should return a tuple with two floats, i.e. (recall, true positive rate).

def answer_five():
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import precision_recall_curve, roc_curve
    #import matplotlib.pyplot as plt
    #%matplotlib notebook
​
    clf = LogisticRegression().fit(X_train, y_train)
    y_scores_lr = clf.decision_function(X_test)
    precision, recall, thresholds = precision_recall_curve(y_test, y_scores_lr)
​
    plt.figure()
    plt.xlim([0.0, 1.01])
    plt.ylim([0.0, 1.01])
    plt.plot(precision, recall, label='Precision-Recall Curve')
    plt.vlines(0.75, 0, 1)
    plt.xlabel('Precision', fontsize=16)
    plt.ylabel('Recall', fontsize=16)
    plt.axes().set_aspect('equal')
    plt.show()
​
    fpr_lr, tpr_lr, _ = roc_curve(y_test, y_scores_lr)
    plt.figure()
    plt.xlim([-0.01, 1.00])
    plt.ylim([-0.01, 1.01])
    plt.plot(fpr_lr, tpr_lr)
    plt.vlines(0.16, 0, 1)
    plt.xlabel('False Positive Rate', fontsize=16)
    plt.ylabel('True Positive Rate', fontsize=16)
    plt.plot([0, 1], [0, 1], color='navy', lw=3, linestyle='--')
    plt.axes().set_aspect('equal')
    plt.show()
    
    return 0.83, 0.94
Question 6
Perform a grid search over the parameters listed below for a Logisitic Regression classifier, using recall for scoring and the default 3-fold cross validation.

'penalty': ['l1', 'l2']

'C':[0.01, 0.1, 1, 10, 100]

From .cv_results_, create an array of the mean test scores of each parameter combination. i.e.

l1	l2
0.01	?	?
0.1	?	?
1	?	?
10	?	?
100	?	?


This function should return a 5 by 2 numpy array with 10 floats.

Note: do not return a DataFrame, just the values denoted by '?' above in a numpy array. You might need to reshape your raw result to meet the format we are looking for.

grid_clf_acc = GridSearchCV(clf, param_grid = grid_values)
def answer_six():    
    from sklearn.linear_model import LogisticRegression
    from sklearn.model_selection import GridSearchCV
    
    clf = LogisticRegression()
    grid_values = {'C': [0.01, 0.1, 1, 10, 100], 'penalty': ['l1', 'l2']}
    grid_clf_acc = GridSearchCV(clf, param_grid = grid_values, scoring='recall')
    grid_clf_acc.fit(X_train, y_train) 
    results = grid_clf_acc.cv_results_
    result = results['mean_test_score']
    test_scores = np.vstack((results['split0_test_score'], results['split1_test_score'], results['split2_test_score']))
    return test_scores.mean(axis=0).reshape(5, 2)
answer_six()
array([[ 0.99422214,  0.99539001],
       [ 0.99606614,  0.99618907],
       [ 0.99606614,  0.9961276 ],
       [ 0.99588174,  0.9959432 ],
       [ 0.99588174,  0.99588174]])
​
​
#
# Use the following function to help visualize results from the grid search
def GridSearch_Heatmap(scores):
    #%matplotlib notebook
    #import seaborn as sns
    #import matplotlib.pyplot as plt
    plt.figure()    
    sns.heatmap(scores.reshape(5,2), xticklabels=['l1','l2'], yticklabels=[0.01, 0.1, 1, 10, 100])
    plt.yticks(rotation=0);
​
#GridSearch_Heatmap(answer_six())


















































from sklearn.linear_model import LogisticRegression
    from sklearn.model_selection import train_test_split

    train = pd.read_csv('train.csv', encoding="ISO-8859-1", low_memory=False)
    train = train[(train['compliance'] == 1) | (train['compliance'] == 0)]

    columns_to_keep = ['ticket_id', 'fine_amount', 'admin_fee', 'state_fee', 'late_fee', 'discount_amount', 'clean_up_cost', 'judgment_amount']
    X = train[columns_to_keep]
    y  = train['compliance']

    test = pd.read_csv('test.csv', encoding="ISO-8859-1", low_memory=False)
    test = test[columns_to_keep]
    
    X_train, X_test, y_train, y_test = train_test_split(X, y)
    logit = LogisticRegression().fit(X_train, y_train)

    y_score_logit = logit.decision_function(test)

    probs = logit.predict_proba(test)[:,1]
    test['prob'] = probs
    output = test.set_index('ticket_id')['prob']
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
Assignment 3
In this assignment you will explore text message data and create models to predict if a message is spam or not.

import pandas as pd
import numpy as np
​
spam_data = pd.read_csv('spam.csv')
​
spam_data['target'] = np.where(spam_data['target']=='spam',1,0)
spam_data.head(10)
text	target
0	Go until jurong point, crazy.. Available only ...	0
1	Ok lar... Joking wif u oni...	0
2	Free entry in 2 a wkly comp to win FA Cup fina...	1
3	U dun say so early hor... U c already then say...	0
4	Nah I don't think he goes to usf, he lives aro...	0
5	FreeMsg Hey there darling it's been 3 week's n...	1
6	Even my brother is not like to speak with me. ...	0
7	As per your request 'Melle Melle (Oru Minnamin...	0
8	WINNER!! As a valued network customer you have...	1
9	Had your mobile 11 months or more? U R entitle...	1
spa = len(spam_data[spam_data['target'] == 1]) 
notspa = len(spam_data[spam_data['target'] == 0])
​
spa / notspa * 100
15.481865284974095
from sklearn.model_selection import train_test_split
​
​
X_train, X_test, y_train, y_test = train_test_split(spam_data['text'], 
                                                    spam_data['target'], 
                                                    random_state=0)
Question 1
What percentage of the documents in spam_data are spam?

This function should return a float, the percent value (i.e. ratio∗100ratio∗100).

def answer_one():
    is_spam = len(spam_data[spam_data['target'] == 1]) 
    all = len(spam_data['target'])
​
    return is_spam / all * 100
answer_one()
13.406317300789663
Question 2
Fit the training data X_train using a Count Vectorizer with default parameters.

What is the longest token in the vocabulary?

This function should return a string.

from sklearn.feature_extraction.text import CountVectorizer
​
def answer_two():
    clf = CountVectorizer().fit(X_train)
    names = clf.get_feature_names()
    sorted_names = sorted(names, key=len)
    
    return sorted_names[-1]
answer_two()
'com1win150ppmx3age16subscription'
Question 3
Fit and transform the training data X_train using a Count Vectorizer with default parameters.

Next, fit a fit a multinomial Naive Bayes classifier model with smoothing alpha=0.1. Find the area under the curve (AUC) score using the transformed test data.

This function should return the AUC score as a float.

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import roc_auc_score
from sklearn.feature_extraction.text import CountVectorizer
​
def answer_three():
    clf = CountVectorizer().fit(X_train)
    X_train_vectorized = clf.transform(X_train)
    clfNB = MultinomialNB(alpha=0.1).fit(X_train_vectorized, y_train)
    predictions = clfNB.predict(clf.transform(X_test))
    
    return roc_auc_score(y_test, predictions)
answer_three()
0.97208121827411165
Question 4
Fit and transform the training data X_train using a Tfidf Vectorizer with default parameters.

What 20 features have the smallest tf-idf and what 20 have the largest tf-idf?

Put these features in a two series where each series is sorted by tf-idf value and then alphabetically by feature name. The index of the series should be the feature name, and the data should be the tf-idf.

The series of 20 features with smallest tf-idfs should be sorted smallest tfidf first, the list of 20 features with largest tf-idfs should be sorted largest first.

This function should return a tuple of two series (smallest tf-idfs series, largest tf-idfs series).

from sklearn.feature_extraction.text import TfidfVectorizer
​
def answer_four():
    clf = TfidfVectorizer().fit(X_train)
    X_train_vectorized = clf.transform(X_train)
    
    feature_names = np.array(clf.get_feature_names())
    
    sorted_tfidf = X_train_vectorized.max(0).toarray()[0].argsort()
    
    smallest_index = feature_names[sorted_tfidf][:20]
    smallest_value = np.sort(X_train_vectorized.max(0).toarray()[0])[:20]
    smallest = pd.DataFrame([smallest_value, smallest_index]).T
    smallest = smallest.sort_values(by=[0, 1]).set_index(1).T
    smallest_columns = smallest.columns
    smallest = pd.Series(smallest_value, index=smallest_columns)
    del smallest.index.name
    
    largest_index = feature_names[sorted_tfidf][:-21:-1]
    largest_value = np.sort(X_train_vectorized.max(0).toarray()[0])[:-21:-1]
    largest = pd.DataFrame([largest_value, largest_index]).T
    largest = largest.sort_values(by=[0, 1], ascending=[False, True]).set_index(1).T
    largest_columns = largest.columns
    largest = pd.Series(largest_value, index=largest_columns)
    del largest.index.name
    
    
    return (smallest, largest)
answer_four()
(aaniye          0.074475
 athletic        0.074475
 chef            0.074475
 companion       0.074475
 courageous      0.074475
 dependable      0.074475
 determined      0.074475
 exterminator    0.074475
 healer          0.074475
 listener        0.074475
 organizer       0.074475
 pest            0.074475
 psychiatrist    0.074475
 psychologist    0.074475
 pudunga         0.074475
 stylist         0.074475
 sympathetic     0.074475
 venaam          0.074475
 diwali          0.091250
 mornings        0.091250
 dtype: float64, 146tf150p    1.000000
 645          1.000000
 anything     1.000000
 anytime      1.000000
 beerage      1.000000
 done         1.000000
 er           1.000000
 havent       1.000000
 home         1.000000
 lei          1.000000
 nite         1.000000
 ok           1.000000
 okie         1.000000
 thank        1.000000
 thanx        1.000000
 too          1.000000
 where        1.000000
 yup          1.000000
 tick         0.980166
 blank        0.932702
 dtype: float64)
Question 5
Fit and transform the training data X_train using a Tfidf Vectorizer ignoring terms that have a document frequency strictly lower than 3.

Then fit a multinomial Naive Bayes classifier model with smoothing alpha=0.1 and compute the area under the curve (AUC) score using the transformed test data.

This function should return the AUC score as a float.

def answer_five():
    clf = TfidfVectorizer(min_df=3).fit(X_train)
    X_train_vectorized = clf.transform(X_train)
    
    clfNB = MultinomialNB(alpha=0.1).fit(X_train_vectorized, y_train)
    
    predicted = clfNB.predict(clf.transform(X_test))
    
    return roc_auc_score(y_test, predicted)
answer_five()
0.94162436548223349
Question 6
What is the average length of documents (number of characters) for not spam and spam documents?

This function should return a tuple (average length not spam, average length spam).

def answer_six():
    spam_data['Length'] = spam_data['text'].apply(lambda x: len(x))
    not_spam_length = np.mean(spam_data['Length'][spam_data['target']==0])
    spam_length = np.mean(spam_data['Length'][spam_data['target']==1])
    
    return (not_spam_length, spam_length)
answer_six()
(71.023626943005183, 138.8661311914324)


The following function has been provided to help you combine new features into the training data:

def add_feature(X, feature_to_add):
    """
    Returns sparse feature matrix with added feature.
    feature_to_add can also be a list of features.
    """
    from scipy.sparse import csr_matrix, hstack
    return hstack([X, csr_matrix(feature_to_add).T], 'csr')
Question 7
Fit and transform the training data X_train using a Tfidf Vectorizer ignoring terms that have a document frequency strictly lower than 5.

Using this document-term matrix and an additional feature, the length of document (number of characters), fit a Support Vector Classification model with regularization C=10000. Then compute the area under the curve (AUC) score using the transformed test data.

This function should return the AUC score as a float.

from sklearn.svm import SVC
​
def answer_seven():
    clf = TfidfVectorizer(min_df=5).fit(X_train)
    
    X_train_vectorized = clf.transform(X_train)    
    X_train_vectorized_with_length = add_feature(X_train_vectorized, X_train.str.len())
    
    X_test_vectorized = clf.transform(X_test)    
    X_test_vectorized_with_length = add_feature(X_test_vectorized, X_test.str.len())
    
    clfSVM = SVC(C=10000).fit(X_train_vectorized_with_length, y_train)
    
    predicted = clfSVM.predict(X_test_vectorized_with_length)
    
    return roc_auc_score(y_test, predicted)
answer_seven()
0.95813668234215565
Question 8
What is the average number of digits per document for not spam and spam documents?

This function should return a tuple (average # digits not spam, average # digits spam).

def answer_eight():
    spam_data['Digits'] = spam_data['text'].apply(lambda x: len([letter for letter in x if letter.isdigit()]))
    digits_not_spam = np.mean(spam_data['Digits'][spam_data['target']==0])
    digits_spam = np.mean(spam_data['Digits'][spam_data['target']==1])
    
    return (digits_not_spam, digits_spam)
answer_eight()
(0.29927461139896372, 15.759036144578314)
​
Question 9
Fit and transform the training data X_train using a Tfidf Vectorizer ignoring terms that have a document frequency strictly lower than 5 and using word n-grams from n=1 to n=3 (unigrams, bigrams, and trigrams).

Using this document-term matrix and the following additional features:

the length of document (number of characters)
number of digits per document
fit a Logistic Regression model with regularization C=100. Then compute the area under the curve (AUC) score using the transformed test data.

This function should return the AUC score as a float.

from sklearn.linear_model import LogisticRegression
​
def answer_nine():
    clf = TfidfVectorizer(min_df=5, ngram_range=(1,3)).fit(X_train)
    
    X_train_vectorized = clf.transform(X_train)
    X_train_vectorized_with_length = add_feature(X_train_vectorized, X_train.str.len())
    X_train_vectorized_with_lenght_and_digits = add_feature(X_train_vectorized_with_length, X_train.apply(lambda x: len([letter for letter in x if letter.isdigit()])))
    
    X_test_vectorized = clf.transform(X_test)
    X_test_vectorized_with_length = add_feature(X_test_vectorized, X_test.str.len())
    X_test_vectorized_with_lenght_and_digits = add_feature(X_test_vectorized_with_length, X_test.apply(lambda x: len([letter for letter in x if letter.isdigit()])))
        
    clfLR = LogisticRegression(C=100).fit(X_train_vectorized_with_lenght_and_digits, y_train)
    
    predictions = clfLR.predict(X_test_vectorized_with_lenght_and_digits)   
    
    return roc_auc_score(y_test, predictions)
answer_nine()
0.96533283533945646
Question 10
What is the average number of non-word characters (anything other than a letter, digit or underscore) per document for not spam and spam documents?

Hint: Use \w and \W character classes

This function should return a tuple (average # non-word characters not spam, average # non-word characters spam).

def answer_ten():
    spam_data['Non-word'] = spam_data['text'].str.findall(r'(\W)').str.len()
    not_spam = np.mean(spam_data['Non-word'][spam_data['target']==0])
    spam = np.mean(spam_data['Non-word'][spam_data['target']==1])
    
    return (not_spam, spam)
answer_ten()
(17.291813471502589, 29.041499330655956)
Question 11
Fit and transform the training data X_train using a Count Vectorizer ignoring terms that have a document frequency strictly lower than 5 and using character n-grams from n=2 to n=5.

To tell Count Vectorizer to use character n-grams pass in analyzer='char_wb' which creates character n-grams only from text inside word boundaries. This should make the model more robust to spelling mistakes.

Using this document-term matrix and the following additional features:

the length of document (number of characters)
number of digits per document
number of non-word characters (anything other than a letter, digit or underscore.)
fit a Logistic Regression model with regularization C=100. Then compute the area under the curve (AUC) score using the transformed test data.

Also find the 10 smallest and 10 largest coefficients from the model and return them along with the AUC score in a tuple.

The list of 10 smallest coefficients should be sorted smallest first, the list of 10 largest coefficients should be sorted largest first.

The three features that were added to the document term matrix should have the following names should they appear in the list of coefficients: ['length_of_doc', 'digit_count', 'non_word_char_count']

This function should return a tuple (AUC score as a float, smallest coefs list, largest coefs list).

def answer_eleven():
    clf = CountVectorizer(min_df=5, ngram_range=(2,5), analyzer='char_wb')
    
    X_train_vectorized = clf.fit_transform(X_train)
    X_train_vectorized_with_length = add_feature(X_train_vectorized, X_train.str.len())
    X_train_vectorized_with_lenght_and_digits = add_feature(X_train_vectorized_with_length, X_train.apply(lambda x: len([letter for letter in x if letter.isdigit()])))
    X_train_vectorized_with_lenght_and_digits_and_nonwords = add_feature(X_train_vectorized_with_lenght_and_digits, X_train.str.findall(r'(\W)').str.len())
    
    X_test_vectorized = clf.transform(X_test)
    X_test_vectorized_with_length = add_feature(X_test_vectorized, X_test.str.len())
    X_test_vectorized_with_lenght_and_digits = add_feature(X_test_vectorized_with_length, X_test.apply(lambda x: len([letter for letter in x if letter.isdigit()])))
    X_test_vectorized_with_lenght_and_digits_and_nonwords = add_feature(X_test_vectorized_with_lenght_and_digits, X_test.str.findall(r'(\W)').str.len())
    
    clfLR = LogisticRegression(C=100).fit(X_train_vectorized_with_lenght_and_digits_and_nonwords, y_train)
    
    prediction = clfLR.predict(X_test_vectorized_with_lenght_and_digits_and_nonwords)
    
    score = roc_auc_score(prediction, y_test)
    
    feature_names = np.array(clf.get_feature_names() +  ['length_of_doc', 'digit_count', 'non_word_char_count'])
    
    sorted_coef_index = clfLR.coef_[0].argsort()
    smallest_coef = feature_names[sorted_coef_index[:10]]
    largest_coef = feature_names[sorted_coef_index[:-11:-1]]
    
    return (score, list(smallest_coef), list(largest_coef))
answer_eleven()
(0.9914366108841286,
 ['. ', '..', '? ', ' i', ' y', ' go', ':)', ' h', 'go', ' m'],
 ['digit_count', 'ne', 'ia', 'co', 'xt', ' ch', 'mob', ' x', 'ww', 'ar'])
    
    
